{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU, PReLU, ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv('../input/ccdata/churnArticle1.txt', sep=\",\")\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x = \"Churn\", data = df)\n",
    "df.loc[:, 'Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the non-numeric data into numeric data.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoded = df.apply(lambda x: LabelEncoder().fit_transform(x) if x.dtype == 'object' else x)\n",
    "encoded.head()\n",
    "plt.figure(figsize =(20,20))\n",
    "Corr=encoded[encoded.columns].corr()\n",
    "sns.heatmap(Corr,annot=True)\n",
    "X = encoded.iloc[:, 0:19]\n",
    "y = encoded.Churn\n",
    "X.head()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)\n",
    "\n",
    "# Feature Importance\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "k=1\n",
    "seed=100\n",
    "sm= SMOTE(sampling_strategy='auto', k_neighbors=k, random_state=seed)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "X=X_res\n",
    "y=y_res\n",
    "#### [0-1] normalizasyon y√∂ntemi\n",
    "#from sklearn.preprocessing import Normalizer\n",
    "#nm=Normalizer()\n",
    "#X=nm.fit_transform(X)\n",
    "\n",
    "## [-1,1] normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SC = StandardScaler()\n",
    "X = SC.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#print length of X_train, X_test, y_train, y_test\n",
    "print (\"X_train: \", len(X_train))\n",
    "print(\"X_test: \", len(X_test))\n",
    "print(\"y_train: \", len(y_train))\n",
    "print(\"y_test: \", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_metrics as km\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, LeakyReLU, BatchNormalization, Dropout\n",
    "from keras.activations import relu, sigmoid\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_model(layers=(6, 3, 3),activation='relu', learn_rate=0.001,momentum=0.0, init='uniform'):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes, kernel_initializer = init, activation=activation,input_dim = X_train.shape[1]))\n",
    "            model.add(Dropout(0.1))\n",
    "        else:\n",
    "            model.add(Dense(nodes, kernel_initializer = init, activation=activation))\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(1, kernel_initializer = init, activation = 'sigmoid'))\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)                    \n",
    "    # Compiling the ANN\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[km.binary_f1_score(),\n",
    "                                                                           km.binary_precision(),km.binary_recall(),\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "layers = [(6, 3, 3), (10, 10), (45, 30, 15)]\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid']\n",
    "batch_sizes=[10, 20, 40, 60, 80, 100]\n",
    "epchos=[10, 50, 100]\n",
    "learn_rate = [0.001, 0.1, 0.3, 0.6]\n",
    "momentum = [0.0, 0.4, 0.8, 0.9]\n",
    "init = ['uniform', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_random = dict(layers=layers, activation=activation,learn_rate=learn_rate, momentum=momentum, \n",
    "                    init=init, batch_size = batch_sizes, epochs=epchos)\n",
    "model = RandomizedSearchCV(estimator = clf, param_distributions = param_random, \n",
    "                           n_iter = 5, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=10):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "grid_result=model.fit(X_train, y_train)\n",
    "\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(model.cv_results_['params'])))\n",
    "\n",
    "report(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "best_parameters = model.best_params_\n",
    "best_accuracy = model.best_score_\n",
    "print('Best Parameters after tuning: {}'.format(best_parameters))\n",
    "print('Best Accuracy after tuning: {}'.format(best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RSM based approach - Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_metrics as km\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(10, kernel_initializer = 'he_normal',activation='softsign',input_dim = 19))\n",
    "#classifier.add(Dropout(0.1))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(10, kernel_initializer = 'he_normal',activation='softsign'))\n",
    "#classifier.add(Dropout(0.1))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "#classifier.add(Dense(15, kernel_initializer = 'he_normal',activation='tanh'))\n",
    "#classifier.add(Dropout(0.1))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, kernel_initializer = 'glorot_normal', activation = 'sigmoid'))\n",
    "\n",
    "optimizer = SGD(lr=0.38, momentum=0.14) \n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=[km.binary_f1_score(),\n",
    "                                                                           km.binary_precision(),km.binary_recall(),\"accuracy\"])\n",
    "\n",
    "model_history = classifier.fit(X_train, y_train, validation_split=0.20, batch_size = 40, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# predict probabilities for test set\n",
    "yhat_probs = classifier.predict(X_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = classifier.predict_classes(X_test, verbose=0)\n",
    "\n",
    "# predict probabilities for test set\n",
    "yhat_probs = classifier.predict(X_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = classifier.predict_classes(X_test, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %.4f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %.4f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %.4f' % f1)\n",
    "#roc-auc\n",
    "roc_auc = roc_auc_score(y_test, yhat_classes)\n",
    "print('ROC-AUC score: %.4f' % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt \n",
    "  \n",
    "workbook = xlwt.Workbook()  \n",
    "  \n",
    "sheet = workbook.add_sheet(\"Sheet3\") \n",
    "  \n",
    "# Specifying style \n",
    "style = xlwt.easyxf('font: bold 1') \n",
    "  \n",
    "# Specifying column \n",
    "for i in range(20):\n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(10, kernel_initializer = 'he_normal',activation='softsign',input_dim = 19))\n",
    "    #classifier.add(Dropout(0.1))\n",
    "\n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(Dense(10, kernel_initializer = 'he_normal',activation='softsign'))\n",
    "    #classifier.add(Dropout(0.1))\n",
    "\n",
    "    # Adding the third hidden layer\n",
    "    #classifier.add(Dense(15, kernel_initializer = 'he_normal',activation='tanh'))\n",
    "    #classifier.add(Dropout(0.1))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(1, kernel_initializer = 'glorot_normal', activation = 'sigmoid'))\n",
    "\n",
    "    optimizer = SGD(lr=0.40, momentum=0.14) \n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=[km.binary_f1_score(),\n",
    "                                                                           km.binary_precision(),km.binary_recall(),\"accuracy\"])\n",
    "\n",
    "    model_history = classifier.fit(X_train, y_train, validation_split=0.20, batch_size = 40, epochs = 50)\n",
    "    # predict probabilities for test set\n",
    "    yhat_probs = classifier.predict(X_test, verbose=0)\n",
    "    # predict crisp classes for test set\n",
    "    yhat_classes = classifier.predict_classes(X_test, verbose=0)\n",
    "\n",
    "    # predict probabilities for test set\n",
    "    yhat_probs = classifier.predict(X_test, verbose=0)\n",
    "    # predict crisp classes for test set\n",
    "    yhat_classes = classifier.predict_classes(X_test, verbose=0)\n",
    "    # reduce to 1d array\n",
    "    yhat_probs = yhat_probs[:, 0]\n",
    "    yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    roc_auc = roc_auc_score(y_test, yhat_classes)\n",
    "    sheet.write(i, 0, accuracy, style)\n",
    "    sheet.write(i, 1, precision, style)\n",
    "    sheet.write(i, 2, recall, style)\n",
    "    sheet.write(i, 3, f1, style)\n",
    "    sheet.write(i, 4, roc_auc, style)\n",
    "    workbook.save(\"set1_cc.xls\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
